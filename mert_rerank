#!/usr/bin/env python
import optparse
import sys
import bleu

def dot_prod(a, b):
    assert (len(a) == len(b))
    product = 0.0
    for i in xrange(len(a)):
        product += (a[i] * b[i])

    return product


def scalar_mult(s, a):
    b = []
    for i in xrange(len(a)):
        b.append(a[i] * s)

    return tuple(b)


def vector_add(a, b):
    assert (len(a) == len(b))
    c = []
    for i in xrange(len(a)):
        c.append(a[i] + b[i])

    return tuple(c)


def vector_sub(a, b):
    assert (len(a) == len(b))
    c = []
    for i in xrange(len(a)):
        c.append(a[i] - b[i])

    return tuple(c)


class MERT:
    def __init__(self):
        self.initiated = True

    # Based on Powell's Quadratically Convergent Method on page 511 of
    # Numerical Recipes 3rd Edition: The Art of Scientific Computing
    def run_mert_powell(self, ref, data, w, directions=[(1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0)]):
        p = {0: w}
        dim = len(directions)

        prev = float('inf')
        while (True):
            sys.stderr.write("Performing iteration of Powell's Method\n")
            for i, d in enumerate(directions):
                p[i+1] = self.line_search(ref, data, p[i] , d)

            for j in xrange(dim - 1):
                directions[j] = directions[j+1]

            directions[dim-1] = vector_sub(p[dim], p[0])
            
            p[0] = self.line_search(ref, data, p[dim], directions[dim-1]) 
            new_score = self.evaluate(ref, data, p[0])
            sys.stderr.write("Finished one iteration of Powell's Method. \nImproved score from " + str(prev) + " to " + str(new_score) + ".\n\n")
            if new_score >= prev:
                break

            prev = new_score

        return p[0]

    def line_search(self, ref, data, w, d):
        sys.stderr.write("Performing line_search with direction " + str(d) + ".\n")
        assert (len(w) == len(d))
        epsilon = 0.0 # ???TODO
        m = {}
        b = {}
        intercepts = []
        for candidates in data:
            # Identifying the points e as their num, hopefully more efficient
            for (e, hypo, features) in candidates:
                m[hypo] = dot_prod(features, d)
                b[hypo] = dot_prod(features, w)

            best_n = None
            max_n_value = 0.0
            for (num, hypo, features) in candidates:
                if m[hypo] > max_n_value:
                    best_n = hypo
                    max_n_value = m[hypo]
                elif m[hypo] == max_n_value:
                    # b[e] breaks ties
                    if b[hypo] > b[best_n]:
                        best_n = hypo
                        max_n_value = m[hypo]
            assert (best_n is not None)

            while (True):
                best_np1 = None
                min_np1_value = float("inf")
                # Looking for argmin this time
                for (num, hypo, features) in candidates:
                    denominator = m[hypo] - m[best_n]
                    if denominator == 0:
                        continue  # will raise a divide-by-zero error

                    i = (b[best_n] - b[hypo]) / (denominator)
                    val = max(0.0, i)
                    if val < min_np1_value:
                        best_np1 = hypo
                        min_np1_value = val

                assert (best_np1 is not None)

                int_cand = (b[best_n] - b[best_np1]) / (m[best_np1] - m[best_n])
                intercept = max(0.0, int_cand)
                if intercept > 0.0:
                    if intercept in intercepts:
                        break
                    intercepts.append(intercept)
                    best_n = best_np1
                else:
                    break

        # intercepts.append(max(intercepts) + (2 * epsilon)) TODO 
        i_best = None
        min_i_score = float("inf")
        for j, intercept in enumerate(intercepts):
            if j % 50 == 0:
                sys.stderr.write(str(j) + " out of " + str(len(intercepts)) + " intercepts tested.\n")
            weights = vector_add(w, scalar_mult(intercept, d))
            val = 1.0 - self.evaluate(ref, data, weights)
            if val < min_i_score:
                i_best = intercept
                min_i_score = val

        assert (i_best is not None)
        
        sys.stderr.write("Best intercept score: " + str(1.0 - min_i_score) + ".\n")

        return vector_add(w, scalar_mult(i_best, d))

    def evaluate(self, ref, data, w):
        stats = [0 for i in xrange(10)]
        best_picks = []
        for j, candidates in enumerate(data):
            (best_score, best) = (-1e300, '')
            for (e, hypo, features) in candidates:
                score = 0.0
                for k in xrange(len(features)):
                    score += w[k] * features[k]
                if score > best_score:
                    (best_score, best) = (score, hypo)

            assert (best is not None)

            best_picks.append(best)

        best_picks_split = [line.strip().split() for line in best_picks]

        for (r,h) in zip(ref, best_picks_split):
            stats = [sum(scores) for scores in zip(stats, bleu.bleu_stats(h,r))]

        return bleu.bleu(stats)


if __name__ == '__main__':
    optparser = optparse.OptionParser()
    optparser.add_option("-r", "--reference", dest="reference", default="data/dev.ref", help="Target language reference sentences")
    optparser.add_option("-k", "--kbest-list", dest="input", default="data/dev+test.100best", help="100-best translation lists")
    optparser.add_option("-l", "--lm", dest="lm", default=-1.0, type="float", help="Language model weight")
    optparser.add_option("-t", "--tm1", dest="tm1", default=-0.5, type="float", help="Translation model p(e|f) weight")
    optparser.add_option("-s", "--tm2", dest="tm2", default=-0.5, type="float", help="Lexical translation model p_lex(f|e) weight")
    (opts, _) = optparser.parse_args()

    ref = [line.strip().split() for line in open(opts.reference)]
    all_hyps = [pair.split(' ||| ') for pair in open(opts.input)]
    num_sents = len(all_hyps) / 100
    mert = MERT()   # M.E.R.T. engine
    
    data = []
    for s in xrange(0, num_sents):
        hyps_for_one_sent = all_hyps[s * 100:s * 100 + 100]
        candidates = []
        for (num, hyp, feats) in hyps_for_one_sent:
            features = []
            for feat in feats.split(' '):
                (k, v) = feat.split('=')
                features.append(float(v))
            candidates.append((num, hyp, tuple(features)))
        data.append(candidates)

    w = (float(opts.lm), float(opts.tm1), float(opts.tm2))
    best_w = mert.run_mert_powell(ref, data, w)

    for candidates in data:
        for (e, hyp, features) in candidates:
            score = 0.0
            (best_score, best) = (-1e300, '')
            for k in xrange(len(features)):
                score += best_w[k] * features[k]
            if score > best_score:
                (best_score, best) = (score, hyp)

        try: 
            sys.stdout.write("%s\n" % best)
        except (Exception):
            sys.exit(1)

